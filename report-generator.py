#!/usr/bin/env python3
"""
PR Review Report Generator

Generates a comprehensive markdown report from all analysis results
"""

import json
import sys
import argparse
from datetime import datetime
from typing import Dict, Any, List

class ReportGenerator:
    def __init__(self):
        self.template = """# 🤖 AI-Powered Pull Request Review

> **Generated on:** {timestamp}  
> **Review Status:** {status}  
> **Overall Score:** {overall_score}/100

## 📊 Summary

{summary_section}

## 🔍 Detailed Analysis

### 🔒 Security Analysis
{security_section}

### 🎨 Code Style & Linting
{lint_section}

### ⚡ Code Quality & Complexity
{quality_section}

### 🧠 AI-Powered Insights
{ai_section}

## 📋 Action Items

{action_items}

## 📈 Recommendations

{recommendations}

---
*This review was automatically generated by the AI PR Review system.*
"""

    def generate_report(self, syntax_file: str, security_file: str, lint_file: str, 
                       quality_file: str, ai_file: str, output_file: str):
        """Generate comprehensive PR review report"""
        
        # Load all result files
        results = {
            'syntax': self._load_json(syntax_file),
            'security': self._load_json(security_file),
            'lint': self._load_json(lint_file),
            'quality': self._load_json(quality_file),
            'ai': self._load_json(ai_file)
        }
        
        # Generate report sections
        status = self._determine_status(results)
        overall_score = self._calculate_overall_score(results)
        summary_section = self._generate_summary_section(results)
        security_section = self._generate_security_section(results['security'])
        lint_section = self._generate_lint_section(results['lint'])
        quality_section = self._generate_quality_section(results['quality'])
        ai_section = self._generate_ai_section(results['ai'])
        action_items = self._generate_action_items(results)
        recommendations = self._generate_recommendations(results, overall_score)
        
        # Generate final report
        report = self.template.format(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S UTC"),
            status=status,
            overall_score=overall_score,
            summary_section=summary_section,
            security_section=security_section,
            lint_section=lint_section,
            quality_section=quality_section,
            ai_section=ai_section,
            action_items=action_items,
            recommendations=recommendations
        )
        
        # Save report
        with open(output_file, 'w') as f:
            f.write(report)
        
        print(f"✅ Generated comprehensive review report: {output_file}")
        return report

    def _load_json(self, file_path: str) -> Dict[str, Any]:
        """Load JSON file safely"""
        try:
            with open(file_path, 'r') as f:
                return json.load(f)
        except (FileNotFoundError, json.JSONDecodeError) as e:
            print(f"⚠️ Could not load {file_path}: {e}")
            return {}

    def _determine_status(self, results: Dict[str, Dict]) -> str:
        """Determine overall review status"""
        security = results.get('security', {})
        quality = results.get('quality', {})
        syntax = results.get('syntax', {})
        
        # Check for critical issues
        if (syntax.get('errors', []) or 
            security.get('errors', 0) > 0 or 
            quality.get('critical', 0) > 0):
            return "❌ **NEEDS ATTENTION** - Critical issues found"
        
        # Check for warnings
        total_warnings = (
            len(syntax.get('warnings', [])) +
            security.get('warnings', 0) +
            quality.get('warnings', 0)
        )
        
        if total_warnings > 5:
            return "⚠️ **REVIEW SUGGESTED** - Multiple issues found"
        elif total_warnings > 0:
            return "✅ **APPROVED WITH SUGGESTIONS** - Minor issues found"
        else:
            return "✅ **APPROVED** - No major issues detected"

    def _calculate_overall_score(self, results: Dict[str, Dict]) -> int:
        """Calculate overall score from all analyses"""
        scores = []
        
        # Security score
        security = results.get('security', {})
        if 'security_score' in security:
            scores.append(security['security_score'])
        
        # Lint score
        lint = results.get('lint', {})
        if 'lint_score' in lint:
            scores.append(lint['lint_score'])
        
        # Quality score
        quality = results.get('quality', {})
        if 'overall_score' in quality:
            scores.append(quality['overall_score'])
        
        # AI score
        ai = results.get('ai', {})
        if ai and 'summary' in ai:
            ai_score = ai['summary'].get('average_score', 70)
            scores.append(ai_score)
        
        return round(sum(scores) / len(scores)) if scores else 70

    def _generate_summary_section(self, results: Dict[str, Dict]) -> str:
        """Generate summary section"""
        syntax = results.get('syntax', {})
        security = results.get('security', {})
        lint = results.get('lint', {})
        quality = results.get('quality', {})
        ai = results.get('ai', {})
        
        summary = f"""
| Metric | Status | Details |
|--------|--------|---------|
| **Syntax Errors** | {"❌" if syntax.get('errors') else "✅"} | {len(syntax.get('errors', []))} errors, {len(syntax.get('warnings', []))} warnings |
| **Security Issues** | {"❌" if security.get('errors', 0) > 0 else "✅"} | {security.get('errors', 0)} high severity, {security.get('warnings', 0)} medium/low |
| **Code Style** | {"❌" if lint.get('errors', 0) > 0 else "✅"} | {lint.get('errors', 0)} errors, {lint.get('warnings', 0)} warnings |
| **Code Quality** | {"❌" if quality.get('critical', 0) > 0 else "✅"} | Score: {quality.get('overall_score', 'N/A')}/100 |
| **AI Analysis** | {"🤖" if ai else "⚠️"} | {ai.get('summary', {}).get('total_files_analyzed', 0) if ai else 0} files analyzed |
"""
        
        return summary

    def _generate_security_section(self, security: Dict[str, Any]) -> str:
        """Generate security analysis section"""
        if not security:
            return "⚠️ Security analysis not available."
        
        score = security.get('security_score', 0)
        vulnerabilities = security.get('vulnerabilities', [])
        
        section = f"""
**Security Score: {score}/100**

"""
        
        if not vulnerabilities:
            section += "✅ No security vulnerabilities detected!\n"
            return section
        
        # Group by severity
        high_severity = [v for v in vulnerabilities if v.get('severity') == 'high']
        medium_severity = [v for v in vulnerabilities if v.get('severity') == 'medium']
        low_severity = [v for v in vulnerabilities if v.get('severity') == 'low']
        
        if high_severity:
            section += "### 🔴 High Severity Issues\n\n"
            for vuln in high_severity[:5]:  # Limit to top 5
                section += f"- **{vuln.get('category', 'Unknown').replace('_', ' ').title()}** (Line {vuln.get('line', '?')}): {vuln.get('message', '')}\n"
                if vuln.get('code_snippet'):
                    section += f"  ```\n  {vuln['code_snippet']}\n  ```\n"
        
        if medium_severity:
            section += "\n### 🟡 Medium Severity Issues\n\n"
            for vuln in medium_severity[:3]:  # Limit to top 3
                section += f"- **{vuln.get('category', 'Unknown').replace('_', ' ').title()}** (Line {vuln.get('line', '?')}): {vuln.get('message', '')}\n"
        
        if len(vulnerabilities) > 8:
            section += f"\n*... and {len(vulnerabilities) - 8} more issues. See detailed logs for complete list.*\n"
        
        return section

    def _generate_lint_section(self, lint: Dict[str, Any]) -> str:
        """Generate linting section"""
        if not lint:
            return "⚠️ Linting analysis not available."
        
        score = lint.get('lint_score', 0)
        issues = lint.get('style_issues', [])
        
        section = f"""
**Lint Score: {score}/100**

"""
        
        if not issues:
            section += "✅ No linting issues found!\n"
            return section
        
        # Group by category
        categories = {}
        for issue in issues:
            cat = issue.get('category', 'other')
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(issue)
        
        for category, cat_issues in list(categories.items())[:5]:  # Top 5 categories
            section += f"### 🎨 {category.replace('-', ' ').title()}\n\n"
            for issue in cat_issues[:3]:  # Top 3 issues per category
                severity_icon = "🔴" if issue.get('severity') == 'error' else "🟡"
                section += f"- {severity_icon} **{issue.get('file', '')}** (Line {issue.get('line', '?')}): {issue.get('message', '')}\n"
        
        if len(issues) > 15:
            section += f"\n*... and {len(issues) - 15} more linting issues.*\n"
        
        return section

    def _generate_quality_section(self, quality: Dict[str, Any]) -> str:
        """Generate code quality section"""
        if not quality:
            return "⚠️ Quality analysis not available."
        
        score = quality.get('overall_score', 0)
        summary = quality.get('summary', {})
        
        section = f"""
**Quality Score: {score}/100**

### 📈 Code Metrics

| Metric | Value |
|--------|-------|
| **Average Complexity** | {summary.get('average_complexity', 'N/A')} |
| **Average Lines of Code** | {summary.get('average_loc', 'N/A')} |
| **Maintainability Index** | {summary.get('average_maintainability', 'N/A')}/100 |
| **Total Functions** | {summary.get('total_functions', 'N/A')} |
| **High Complexity Files** | {summary.get('files_with_high_complexity', 'N/A')} |

"""
        
        # Show quality issues from individual files
        metrics = quality.get('quality_metrics', [])
        if metrics:
            section += "### 🔍 Quality Issues\n\n"
            
            for metric in metrics[:3]:  # Top 3 files
                file_issues = metric.get('issues', [])
                if file_issues:
                    section += f"**{metric.get('file', 'Unknown')}:**\n"
                    for issue in file_issues[:2]:  # Top 2 issues per file
                        severity_icon = "🔴" if issue.get('severity') == 'high' else "🟡"
                        section += f"- {severity_icon} {issue.get('message', '')}\n"
                    section += "\n"
        
        return section

    def _generate_ai_section(self, ai: Dict[str, Any]) -> str:
        """Generate AI analysis section"""
        if not ai or not ai.get('detailed_results'):
            return "⚠️ AI analysis not available."
        
        summary = ai.get('summary', {})
        recommendation = ai.get('recommendation', '')
        
        section = f"""
**AI Analysis Summary:**
- **Files Analyzed:** {summary.get('total_files_analyzed', 0)}
- **Files with Issues:** {summary.get('files_with_issues', 0)}
- **Total Issues Found:** {summary.get('total_issues', 0)}
- **Average Score:** {summary.get('average_score', 'N/A')}/100

**AI Recommendation:** {recommendation}

### 🧠 Key AI Insights

"""
        
        # Show top AI-identified issues
        detailed_results = ai.get('detailed_results', [])
        for result in detailed_results[:3]:  # Top 3 files
            analysis = result.get('analysis', {})
            if analysis and analysis.get('issues'):
                section += f"**{result.get('file', 'Unknown')}:**\n"
                for issue in analysis['issues'][:2]:  # Top 2 issues per file
                    severity_icon = "🔴" if issue.get('severity') == 'high' else "🟡"
                    section += f"- {severity_icon} **{issue.get('category', 'General').title()}**: {issue.get('message', '')}\n"
                    if issue.get('suggestion'):
                        section += f"  *Suggestion: {issue['suggestion']}*\n"
                section += "\n"
        
        return section

    def _generate_action_items(self, results: Dict[str, Dict]) -> str:
        """Generate prioritized action items"""
        action_items = []
        
        # Critical syntax errors
        syntax = results.get('syntax', {})
        if syntax.get('errors'):
            action_items.append("🔴 **URGENT**: Fix syntax errors before merging")
        
        # High security issues
        security = results.get('security', {})
        if security.get('errors', 0) > 0:
            action_items.append("🔴 **URGENT**: Address high-severity security vulnerabilities")
        
        # Critical quality issues
        quality = results.get('quality', {})
        if quality.get('critical', 0) > 0:
            action_items.append("🔴 **URGENT**: Refactor code with critical quality issues")
        
        # Medium priority items
        total_warnings = (
            len(syntax.get('warnings', [])) +
            security.get('warnings', 0) +
            quality.get('warnings', 0)
        )
        
        if total_warnings > 0:
            action_items.append(f"🟡 **MEDIUM**: Address {total_warnings} warning(s) when possible")
        
        # AI suggestions
        ai = results.get('ai', {})
        if ai and ai.get('summary', {}).get('total_suggestions', 0) > 0:
            action_items.append("💡 **LOW**: Consider AI-suggested improvements")
        
        if not action_items:
            action_items.append("✅ **No critical action items** - Code looks good!")
        
        return "\n".join([f"{i+1}. {item}" for i, item in enumerate(action_items)])

    def _generate_recommendations(self, results: Dict[str, Dict], overall_score: int) -> str:
        """Generate overall recommendations"""
        recommendations = []
        
        if overall_score >= 85:
            recommendations.append("🎉 **Excellent work!** This code meets high quality standards.")
        elif overall_score >= 70:
            recommendations.append("👍 **Good job!** Minor improvements could make this code even better.")
        elif overall_score >= 50:
            recommendations.append("⚠️ **Needs improvement.** Address the identified issues before merging.")
        else:
            recommendations.append("❌ **Significant issues found.** Major refactoring recommended.")
        
        # Specific recommendations based on analysis
        security = results.get('security', {})
        if security.get('security_score', 100) < 80:
            recommendations.append("🔒 **Security Focus**: Review security practices and input validation.")
        
        quality = results.get('quality', {})
        if quality.get('overall_score', 100) < 70:
            recommendations.append("📊 **Quality Focus**: Consider breaking down complex functions and reducing code duplication.")
        
        lint = results.get('lint', {})
        if lint.get('lint_score', 100) < 80:
            recommendations.append("🎨 **Style Focus**: Run code formatters and fix linting issues.")
        
        recommendations.append("\n**Next Steps:**")
        recommendations.append("1. Address critical issues first (red items)")
        recommendations.append("2. Consider AI suggestions for improvements")
        recommendations.append("3. Run local tests before requesting re-review")
        recommendations.append("4. Update documentation if needed")
        
        return "\n".join(recommendations)

def main():
    parser = argparse.ArgumentParser(description='Generate PR Review Report')
    parser.add_argument('--syntax-results', default='syntax-results.json')
    parser.add_argument('--security-results', default='security-results.json')
    parser.add_argument('--lint-results', default='lint-results.json')
    parser.add_argument('--quality-results', default='quality-results.json')
    parser.add_argument('--ai-results', default='ai-results.json')
    parser.add_argument('--output', default='pr-review-report.md')
    
    args = parser.parse_args()
    
    generator = ReportGenerator()
    generator.generate_report(
        args.syntax_results,
        args.security_results,
        args.lint_results,
        args.quality_results,
        args.ai_results,
        args.output
    )

if __name__ == '__main__':
    main()
